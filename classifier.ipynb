{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and global parameters\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Global parameters\n",
    "SR = 22050               # Sampling rate (Hz)\n",
    "DURATION = 3             # Duration (seconds) to load from each file\n",
    "FIXED_LENGTH = SR * DURATION  # Fixed number of samples for each audio file\n",
    "N_MFCC = 40              # Number of MFCCs to extract\n",
    "MAX_PAD_LEN = 130        # Maximum number of time frames for MFCC extraction\n",
    "\n",
    "# Directories (change these to your actual directories)\n",
    "clean_dir = '/content/CleanData'\n",
    "noisy_dir = '/content/NoisyData'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Audio Preprocessing Functions\n",
    "\n",
    "def load_audio(file_path, sr=SR, duration=DURATION):\n",
    "    \"\"\"\n",
    "    Load an audio file and pad/truncate to a fixed length.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "    if len(y) < FIXED_LENGTH:\n",
    "        y = np.pad(y, (0, FIXED_LENGTH - len(y)), mode='constant')\n",
    "    else:\n",
    "        y = y[:FIXED_LENGTH]\n",
    "    return y\n",
    "\n",
    "def extract_mfcc(y, sr=SR, n_mfcc=N_MFCC, max_pad_len=MAX_PAD_LEN):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from an audio signal.\n",
    "    Returns an array of shape (n_mfcc, max_pad_len).\n",
    "    \"\"\"\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    if mfcc.shape[1] < max_pad_len:\n",
    "        pad_width = max_pad_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_pad_len]\n",
    "    return mfcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load file paths and build dataset arrays\n",
    "\n",
    "# Get lists of file paths\n",
    "clean_files = glob.glob(os.path.join(clean_dir, '*.mp3'))\n",
    "noisy_files = glob.glob(os.path.join(noisy_dir, '*.mp3'))\n",
    "\n",
    "print(f\"Found {len(clean_files)} clean files.\")\n",
    "print(f\"Found {len(noisy_files)} noisy files.\")\n",
    "\n",
    "# Build lists for features (X) and labels (y)\n",
    "X = []  # MFCC features\n",
    "y = []  # Labels: 0 for clean, 1 for noisy\n",
    "\n",
    "# Process clean files\n",
    "for file in clean_files:\n",
    "    audio = load_audio(file)\n",
    "    if audio is not None:\n",
    "        mfcc = extract_mfcc(audio)\n",
    "        X.append(mfcc)\n",
    "        y.append(0)\n",
    "\n",
    "# Process noisy files\n",
    "for file in noisy_files:\n",
    "    audio = load_audio(file)\n",
    "    if audio is not None:\n",
    "        mfcc = extract_mfcc(audio)\n",
    "        X.append(mfcc)\n",
    "        y.append(1)\n",
    "\n",
    "X = np.array(X)  # shape: (num_samples, n_mfcc, max_pad_len)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# For PyTorch, we want channels-first. Convert X from (num_samples, n_mfcc, max_pad_len)\n",
    "# to (num_samples, 1, n_mfcc, max_pad_len)\n",
    "X = np.expand_dims(X, axis=1)\n",
    "print(\"X shape after adding channel dimension:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create PyTorch Dataset and DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X is expected shape: (num_samples, 1, n_mfcc, max_pad_len)\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "val_dataset = AudioDataset(X_val, y_val)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Build the CNN Model in PyTorch\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # input_shape: (1, n_mfcc, max_pad_len)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # -> (16, n_mfcc, max_pad_len)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # -> (16, n_mfcc/2, max_pad_len/2)\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # -> (32, n_mfcc/2, max_pad_len/2)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # -> (32, n_mfcc/4, max_pad_len/4)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # -> (64, n_mfcc/4, max_pad_len/4)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)   # -> (64, n_mfcc/8, max_pad_len/8)\n",
    "        )\n",
    "        # Calculate flattened feature size\n",
    "        # Assume n_mfcc and max_pad_len are divisible by 8.\n",
    "        self.flatten_size = 64 * (input_shape[1] // 8) * (input_shape[2] // 8)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Binary classification: noisy vs. clean\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "input_shape = X_train.shape[1:]  # (1, n_mfcc, max_pad_len)\n",
    "model = CNNClassifier(input_shape)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Training loop\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "EPOCHS = 30\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device).unsqueeze(1)  # shape: (batch_size, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device).unsqueeze(1)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            running_loss += loss.item() * batch_X.size(0)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    val_loss = running_loss / len(val_dataset)\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \" +\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Plot training history\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title(\"Loss over epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.title(\"Accuracy over epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Inference functions\n",
    "\n",
    "def inference_preprocess(file_path, sr=SR):\n",
    "    \"\"\"\n",
    "    Load an audio file and split it into segments of FIXED_LENGTH samples.\n",
    "    Returns a numpy array of MFCC batches with shape (num_segments, 1, n_mfcc, max_pad_len).\n",
    "    \"\"\"\n",
    "    audio = load_audio(file_path, sr=sr)\n",
    "    if audio is None:\n",
    "        return None\n",
    "    audio_len = len(audio)\n",
    "    segments = []\n",
    "    # Use non-overlapping segments\n",
    "    for i in range(0, audio_len - FIXED_LENGTH + 1, FIXED_LENGTH):\n",
    "        segment = audio[i:i+FIXED_LENGTH]\n",
    "        mfcc = extract_mfcc(segment)\n",
    "        mfcc = mfcc[..., np.newaxis]  # shape: (n_mfcc, max_pad_len, 1)\n",
    "        segments.append(mfcc)\n",
    "    # Handle leftover (pad if necessary)\n",
    "    if audio_len % FIXED_LENGTH != 0:\n",
    "        segment = audio[-FIXED_LENGTH:]\n",
    "        mfcc = extract_mfcc(segment)\n",
    "        mfcc = mfcc[..., np.newaxis]\n",
    "        segments.append(mfcc)\n",
    "    \n",
    "    segments = np.stack(segments)\n",
    "    # Rearrange to channels-first: (num_segments, 1, n_mfcc, max_pad_len)\n",
    "    segments = np.transpose(segments, (0, 3, 1, 2))\n",
    "    return segments\n",
    "\n",
    "def predict_audio(file_path, model, sr=SR):\n",
    "    \"\"\"\n",
    "    Run inference on a single audio file.\n",
    "    Returns the predicted class for each segment and overall majority vote.\n",
    "    \"\"\"\n",
    "    segments = inference_preprocess(file_path, sr)\n",
    "    if segments is None:\n",
    "        return None, None\n",
    "    segments = torch.from_numpy(segments).float().to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(segments)  # shape: (num_segments, 1)\n",
    "    predictions = predictions.cpu().numpy().flatten()\n",
    "    predicted_labels = (predictions > 0.5).astype(int)\n",
    "    # Overall decision via majority vote\n",
    "    final_prediction = np.round(np.mean(predicted_labels)).astype(int)\n",
    "    return final_prediction, predicted_labels\n",
    "\n",
    "# Example inference:\n",
    "test_file = clean_files[0] if len(clean_files) > 0 else None\n",
    "if test_file:\n",
    "    final_pred, batch_preds = predict_audio(test_file, model)\n",
    "    print(f\"Overall prediction for {os.path.basename(test_file)}: {'Noisy' if final_pred==1 else 'Clean'}\")\n",
    "    print(\"Batch predictions:\", batch_preds)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
